{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from vocab import vocab, END_TOKEN, START_TOKEN, PADDING_TOKEN, UNKNOWN_TOKEN\n",
    "from helpers import readLines\n",
    "from load_data import load_data_evaluate, load_data_training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import json\n",
    "\n",
    "PHRASE_SIZE = 50\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, type_vocab, value_vocab, hidden_size, embedding_size):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.typeEmbedding = nn.Embedding(len(type_vocab), embedding_size, device=device)\n",
    "    self.valueEmbedding = nn.Embedding(len(value_vocab), embedding_size, device=device)\n",
    "    self.positionEmbedding = nn.Embedding(PHRASE_SIZE, 10, device=device)\n",
    "    \n",
    "    self.gru = nn.GRU(embedding_size * 2 + 10, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    E_type_out = self.typeEmbedding(inputs[0])\n",
    "    E_value_out = self.valueEmbedding(inputs[1])\n",
    "    E_pos_out = self.positionEmbedding(inputs[2])\n",
    "\n",
    "    output = torch.cat((E_type_out, E_pos_out, E_value_out), dim=1).view(1, BATCH_SIZE, -1)\n",
    "\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size, device=device)\n",
    "    \n",
    "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    output = self.embedding(input)\n",
    "\n",
    "    output = F.relu(output)\n",
    "    \n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    output = self.softmax(self.out(output[0]))\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=PHRASE_SIZE):\n",
    "    super(AttnDecoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.dropout_p = dropout_p\n",
    "    self.max_length = max_length\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "    self.dropout = nn.Dropout(self.dropout_p)\n",
    "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "  def forward(self, input, hidden, encoder_outputs):\n",
    "    embedded = self.embedding(input).view(1, BATCH_SIZE, -1)\n",
    "    embedded = self.dropout(embedded)\n",
    "\n",
    "    # print(embedded.shape, hidden.shape)\n",
    "    # print(embedded[0].shape, hidden[0].shape)\n",
    "\n",
    "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "    attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "    output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "\n",
    "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "    return output, hidden, attn_weights\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMING & PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "  m = math.floor(s / 60)\n",
    "  s -= m * 60\n",
    "  return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "  now = time.time()\n",
    "  s = now - since\n",
    "  es = s / (percent)\n",
    "  rs = es - s\n",
    "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # this locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=0.01)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.plot(points)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected 20.0 pairs out of 78523 available\n",
      "------------------------\n",
      "loading data: 2/20.0 (10%)\n",
      "loading data: 4/20.0 (20%)\n",
      "loading data: 6/20.0 (30%)\n",
      "loading data: 8/20.0 (40%)\n",
      "loading data: 10/20.0 (50%)\n",
      "loading data: 12/20.0 (60%)\n",
      "loading data: 14/20.0 (70%)\n",
      "loading data: 16/20.0 (80%)\n",
      "loading data: 18/20.0 (90%)\n",
      "loading data: 20/20.0 (100%)\n",
      "------------------------\n",
      "pairs: 20, total articles: 100\n",
      "batch size: 5, phrase size: 50\n",
      "input shape:  torch.Size([3, 5, 50])\n",
      "output shape:  torch.Size([5, 50])\n"
     ]
    }
   ],
   "source": [
    "type_vocab, value_vocab, token_vocab, pairs = load_data_training(torch, device, 50000, BATCH_SIZE, PHRASE_SIZE, 100/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a very tall man <UNK> / 6' <UNK> nicholas, named after his paternal grandfather, the emperor, was born as the eldest son to grand duke nicholas <UNK> of russia <UNK> and alexandra <UNK> of oldenburg <UNK> on 18 november 1856. his father was the sixth child and third son <EOS> \n",
      "<UNK> scored two goals in the final of the 2013 coupe de france to help bordeaux defeat evian <UNK> 3-2. <UNK> had an excellent 2013-14 ligue 1 season with bordeaux, registering 12 goals in 25 ligue 1 matches and scoring in a 1-1 away draw against toulouse fc, one <EOS> \n",
      "this large <UNK> earns its common name from its dark <UNK> <UNK> <UNK> which perfectly <UNK> it against the <UNK> <UNK> <UNK> upon which it <UNK> <UNK> its body is primarily black, but features brown markings along its long, slender <UNK> it is particularly at the <UNK> between segments, <EOS> \n",
      "the varied habitats support a wide variety of bird species, 34 of which occur in the <UNK> areas alone. species of note include the <UNK> <UNK> eastern ground <UNK> little <UNK> and endangered eastern <UNK> the reserve lies within the <UNK> to <UNK> inlet important bird area, so identified <EOS> \n",
      "the de <UNK> bluff <UNK> is a historic public water supply facility at <UNK> and hazel streets in de <UNK> bluff, arkansas. it contains a <UNK> elevated steel water tower, built in 1936 by the <UNK> moines steel company in conjunction with the public works administration as part of <EOS> \n"
     ]
    }
   ],
   "source": [
    "test = pairs[0]\n",
    "target = test[1]\n",
    "\n",
    "for sentence in target:\n",
    "  s = \"\"\n",
    "  for word in sentence:\n",
    "    s += token_vocab.getWord(word.item()) + \" \"\n",
    "  print(s)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# NOTE: inputs are (1, BATCH_SIZE)\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "  encoder_hidden = encoder.initHidden()\n",
    "\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_length = input_tensor.size(2) # == PHRASE_SIZE\n",
    "  target_length = target_tensor.size(1)\n",
    "\n",
    "  # NOTE: attention\n",
    "  encoder_outputs = torch.zeros(PHRASE_SIZE, encoder.hidden_size, device=device)\n",
    "\n",
    "  loss = 0\n",
    "\n",
    "  for i in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[:,:,i], encoder_hidden)\n",
    "    # NOTE: attention\n",
    "    encoder_outputs[i] = encoder_output[0, 0]\n",
    "\n",
    "  decoder_input = torch.tensor([[type_vocab.getID(START_TOKEN)] for _ in range(BATCH_SIZE)], device=device)\n",
    "  decoder_input = decoder_input.view(1, BATCH_SIZE)\n",
    "\n",
    "  decoder_hidden = encoder_hidden\n",
    "\n",
    "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "  # use_teacher_forcing = False\n",
    "\n",
    "  if use_teacher_forcing:\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(target_length):\n",
    "      # NOTE: attention\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "      loss += criterion(decoder_output, target_tensor[:,di])\n",
    "      decoder_input = target_tensor[:, di].view(1, BATCH_SIZE)  # Teacher forcing\n",
    "      \n",
    "  else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "      # NOTE: attention\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "      topv, topi = decoder_output.topk(1)\n",
    "      # decoder_input = topi.squeeze().detach().view(1, BATCH_SIZE)  # detach from history as input\n",
    "      decoder_input = topi\n",
    "\n",
    "      loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "  # loss.backward()\n",
    "  loss = loss / target_length\n",
    "  # loss = loss / BATCH_SIZE\n",
    "  loss.backward()\n",
    "\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=5e-5):\n",
    "  start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0  # Reset every print_every\n",
    "  plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  training_pairs = [random.choice(pairs) for i in range(n_iters)]\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for iter in range(1, n_iters + 1):\n",
    "    training_pair = training_pairs[iter - 1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "      print_loss_avg = print_loss_total / print_every\n",
    "      print_loss_total = 0\n",
    "      print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                      iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "      plot_loss_avg = plot_loss_total / plot_every\n",
    "      plot_losses.append(plot_loss_avg)\n",
    "      plot_loss_total = 0\n",
    "\n",
    "  \n",
    "  showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = EncoderRNN(type_vocab, value_vocab, 256, 128).to(device)\n",
    "# decoder = DecoderRNN(256, len(token_vocab)).to(device)\n",
    "decoder = AttnDecoderRNN(256, len(token_vocab)).to(device)\n",
    "\n",
    "# ITERS = 200000\n",
    "# trainIters(encoder, decoder, ITERS, print_every=ITERS/10, plot_every=ITERS/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor, max_length=PHRASE_SIZE):\n",
    "  with torch.no_grad():\n",
    "    input_length = input_tensor.size(2) # == max_length\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "      encoder_output, encoder_hidden = encoder(input_tensor[:,:,ei], encoder_hidden)\n",
    "      encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[type_vocab.getID(START_TOKEN)] for _ in range(BATCH_SIZE)], device=device)\n",
    "    decoder_input = decoder_input.view(1, BATCH_SIZE)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = [[] for _ in range(BATCH_SIZE)]\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      \n",
    "      # TODO: decoder attention\n",
    "      # decoder_attentions[di] = decoder_attention.data\n",
    "      # print(\"out\" + str(decoder_output[0:10]))\n",
    "      topv, topi = decoder_output.data.topk(1)\n",
    "      # print(topv)\n",
    "      # print(topi)\n",
    "\n",
    "      # TODO: end tokens dopo fine della frase\n",
    "      for i in range(BATCH_SIZE):\n",
    "        decoded_words[i].append(token_vocab.getWord(topi[i].item()))\n",
    "\n",
    "      decoder_input = topi.squeeze().detach().view(1, BATCH_SIZE)  # detach from history as input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----========= CYCLE 1/1 - 1 iterations =========----\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from calendar import EPOCH\n",
    "from datetime import datetime\n",
    "\n",
    "ITERS = 1\n",
    "EPOCHS = 1\n",
    "SAVE_EVERY = 1\n",
    "\n",
    "test_input = pairs[0][0]\n",
    "\n",
    "now = str(datetime.now().strftime(\"%d.%m_%H.%M\"))\n",
    "\n",
    "with open(f\"output/out-{now}.txt\", 'w', encoding='utf-8') as outfile:\n",
    "  pass\n",
    "\n",
    "with open(f\"output/out-{now}.txt\", 'a', encoding='utf-8') as outfile:\n",
    "  for i in range(EPOCHS):\n",
    "    print(f\"----========= CYCLE {i+1}/{EPOCHS} - {ITERS*(i+1)} iterations =========----\")\n",
    "    outfile.write(f\"\\n----========= CYCLE {i+1}/{EPOCHS} - {ITERS*(i+1)} iterations =========----\\n\\n\")\n",
    "\n",
    "    trainIters(encoder, decoder, ITERS, print_every=ITERS/5, plot_every=ITERS/1000)\n",
    "    outputs, _ = evaluate(encoder, decoder, test_input)\n",
    "    \n",
    "    for i in range(len(outputs)):\n",
    "      outfile.write(\"----------------------\\n\")\n",
    "      predict = \"\"\n",
    "      target = \"\"\n",
    "      for word in outputs[i]:\n",
    "        predict += str(word) + \" \"\n",
    "      outfile.write(predict + \"\\n\")\n",
    "      outfile.write(\"-.... ↑|predict|↑ ....... ↓|target|↓ ....-\\n\")\n",
    "      for word in pairs[0][1][i]:\n",
    "        target += token_vocab.getWord(word.item()) + \" \"\n",
    "      outfile.write(target + \"\\n\")\n",
    "\n",
    "    if i % SAVE_EVERY == 0:\n",
    "      torch.save(encoder, f\"models/encoder_{(i+1)*ITERS}-iters.pt\")\n",
    "      torch.save(decoder, f\"models/decoder_{(i+1)*ITERS}-iters.pt\")\n",
    "      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf21a6fff9624ce691a4fad43c68540179d36da31fc4f80138ac9dffe05c7a89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
