{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMamRpRb_nXd",
        "outputId": "a54fb73f-d4da-4ecc-95ff-7cb12d6ef817"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from vocab import vocab, END_TOKEN, START_TOKEN, PADDING_TOKEN, UNKNOWN_TOKEN\n",
        "from helpers import readLines\n",
        "from load_data import load_data_evaluate, load_data_training, getInputSizeAverage\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import json\n",
        "\n",
        "ENCODER_INPUT_SIZE = 10 # dimensione dell'input dell'encoder (numero di triple tipo-valore-posizione in input)\n",
        "DECODER_OUTPUT_SIZE = 35 # dimensione dell'output del decoder (lunghezza della frase in output)\n",
        "BATCH_SIZE = 3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")\n",
        "\n",
        "base_path = \"data\"\n",
        "# device = \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "rz-0w0LK_nXe"
      },
      "outputs": [],
      "source": [
        "# from load_data import getInputSizeAverage\n",
        "\n",
        "# print(getInputSizeAverage())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "2X27yN6x_nXm"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "def asMinutes(s):\n",
        "  m = math.floor(s / 60)\n",
        "  s -= m * 60\n",
        "  return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "  now = time.time()\n",
        "  s = now - since\n",
        "  es = s / (percent)\n",
        "  rs = es - s\n",
        "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "def asMsecs(s):\n",
        "  sec = math.floor(s)\n",
        "  msec = s * 1000\n",
        "  msec -= sec * 1000\n",
        "  return '%ds %dms' % (sec, msec)\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "  # this locator puts ticks at regular intervals\n",
        "  loc = ticker.MultipleLocator(base=0.01)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  # ax.set_facecolor('pink')\n",
        "  plt.plot(points)\n",
        "  plt.show()\n",
        "\n",
        "def getPlot(points):\n",
        "  plt.figure()\n",
        "  fig, ax = plt.subplots()\n",
        "  # this locator puts ticks at regular intervals\n",
        "  loc = ticker.MultipleLocator(base=0.01)\n",
        "  ax.yaxis.set_major_locator(loc)\n",
        "  # ax.set_facecolor('pink')\n",
        "  plt.plot(points)\n",
        "  return fig\n",
        "\n",
        "def calc_avg_loss(prec_loss, curr_loss, alpha=0.95):\n",
        "  if prec_loss == 0:\n",
        "    return curr_loss\n",
        "  return alpha * prec_loss + (1.0 - alpha) * curr_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "_IAkIwYs_nXo",
        "outputId": "2db5226e-668b-45b3-a08b-4fb75a89c869"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "selected 100 pairs out of 130873 available\n",
            "------------------------\n",
            "loading data: 10/100 (10%)\n",
            "loading data: 20/100 (20%)\n",
            "loading data: 30/100 (30%)\n",
            "loading data: 40/100 (40%)\n",
            "loading data: 50/100 (50%)\n",
            "loading data: 60/100 (60%)\n",
            "loading data: 70/100 (70%)\n",
            "loading data: 80/100 (80%)\n",
            "loading data: 90/100 (90%)\n",
            "loading data: 100/100 (100%)\n",
            "------------------------\n",
            "pairs: 100, total articles: 300\n",
            "batch size: 3, input size: 10, output size: 35\n",
            "input shape:  torch.Size([3, 3, 10])\n",
            "output shape:  torch.Size([3, 35])\n"
          ]
        }
      ],
      "source": [
        "type_vocab, value_vocab, token_vocab, pairs = load_data_training(\n",
        "  torch=torch,\n",
        "  device=device,\n",
        "  vocab_size=5000,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  input_size=ENCODER_INPUT_SIZE,\n",
        "  output_size=DECODER_OUTPUT_SIZE,\n",
        "  pair_amount=100,\n",
        "  path=base_path\n",
        ")\n",
        "\n",
        "def split_data(pairs, train_size=0.8):\n",
        "  train_size = int(train_size * len(pairs))\n",
        "  train_pairs = pairs[:train_size]\n",
        "  test_pairs = pairs[train_size:]\n",
        "  return train_pairs, test_pairs\n",
        "\n",
        "train_pairs, test_pairs = split_data(pairs)\n",
        "\n",
        "# dropout percentuale\n",
        "# criterion se serve logsoftmax\n",
        "\n",
        "# togliere dropout nel test piccolo\n",
        "# se funziona tutto attention weights padding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZBph-eU_nXg"
      },
      "source": [
        "# ENCODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "VwOghJo5_nXi"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "  def __init__(self, type_vocab, value_vocab, hidden_size, embedding_size):\n",
        "    super(EncoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.typeEmbedding = nn.Embedding(len(type_vocab), embedding_size, device=device)\n",
        "    self.valueEmbedding = nn.Embedding(len(value_vocab), embedding_size, device=device)\n",
        "    self.positionEmbedding = nn.Embedding(ENCODER_INPUT_SIZE, 10, device=device)\n",
        "    \n",
        "    self.gru = nn.GRU(embedding_size * 2 + 10, self.hidden_size, device=device)\n",
        "    self.dropout1 = nn.Dropout(0.1)\n",
        "    # self.dropout2 = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, inputs, hidden):\n",
        "    E_type_out = self.typeEmbedding(inputs[0]) # [BATCH, EMBEDDING]\n",
        "    E_value_out = self.valueEmbedding(inputs[1]) # [BATCH, EMBEDDING]\n",
        "    E_pos_out = self.positionEmbedding(inputs[2]) # [BATCH, EMBEDDING]\n",
        "\n",
        "    output = torch.cat((E_type_out, E_pos_out, E_value_out), dim=1).view(1, BATCH_SIZE, -1) # [1, BATCH, EMBEDDING * 2 + 10]\n",
        "\n",
        "    output = self.dropout1(output) # [1, BATCH, EMBEDDING * 2 + 10]\n",
        "\n",
        "    output, hidden = self.gru(output, hidden) # [1, BATCH, EMBEDDING * 2 + 10] (entrambi)\n",
        "\n",
        "    # output = self.dropout2(output)\n",
        "\n",
        "    return output, hidden\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjOZmSDG_nXj"
      },
      "source": [
        "# ATTENTION DECODER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttnCalc(nn.Module):\n",
        "  def __init__(self, hidden_size):\n",
        "    super(AttnCalc, self).__init__()\n",
        "    self.decoderAttnLinear = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    self.attnConv = nn.Conv2d(ENCODER_INPUT_SIZE, ENCODER_INPUT_SIZE, (hidden_size, hidden_size), stride=1, padding=\"same\")\n",
        "    self.cvgConv = nn.Conv2d(ENCODER_INPUT_SIZE, ENCODER_INPUT_SIZE, (1, hidden_size), stride=1, padding=\"same\")\n",
        "\n",
        "    self.v = nn.Parameter(torch.FloatTensor(BATCH_SIZE, hidden_size), requires_grad=True)\n",
        "\n",
        "    self.tanhfeatures = nn.Tanh()\n",
        "\n",
        "  def forward(self, hidden, encoder_outputs, coverage):\n",
        "    # hidden = [1, BATCH, HIDDEN]\n",
        "    # encoder_outputs = [ENCODER_INPUT_SIZE, HIDDEN]\n",
        "    # coverage = [1, ENCODER_INPUT_SIZE]\n",
        "\n",
        "    encoder_features = encoder_outputs.view(BATCH_SIZE, ENCODER_INPUT_SIZE, 1, -1) # [BATCH_SIZE, ENCODER_INPUT_SIZE, 1, HIDDEN]\n",
        "    encoder_features = self.attnConv(encoder_features) #- [BATCH_SIZE, ENCODER_INPUT_SIZE, 1, HIDDEN]\n",
        "\n",
        "    decoder_features = self.decoderAttnLinear(hidden) # [1, BATCH, HIDDEN]\n",
        "    decoder_features = decoder_features.view(BATCH_SIZE, 1, 1, -1) #- [BATCH, 1, 1, HIDDEN]\n",
        "    \n",
        "    coverage_features = coverage.view(BATCH_SIZE, ENCODER_INPUT_SIZE, 1, -1) # [BATCH_SIZE, ENCODER_INPUT_SIZE, 1, 1]\n",
        "    coverage_features = self.cvgConv(coverage_features) #- [BATCH_SIZE, ENCODER_INPUT_SIZE, 1, 1]\n",
        "\n",
        "    # [1, ENCODER_INPUT_SIZE, 1, HIDDEN] + [BATCH, 1, 1, HIDDEN] + [1, ENCODER_INPUT_SIZE, 1, 1]\n",
        "    attn_features = encoder_features + decoder_features + coverage_features # [BATCH, ENCODER_INPUT_SIZE, 1, HIDDEN]\n",
        "    attn_features = attn_features.view(BATCH_SIZE, ENCODER_INPUT_SIZE, -1) # [BATCH, ENCODER_INPUT_SIZE, HIDDEN]\n",
        "    attn_features = self.tanhfeatures(attn_features) #- [BATCH, ENCODER_INPUT_SIZE, HIDDEN]\n",
        "\n",
        "    temp_v = self.v.unsqueeze(2) #- [BATCH, HIDDEN, 1]\n",
        "    attn_weights = torch.bmm(attn_features, temp_v) # [BATCH, ENCODER_INPUT_SIZE, 1]\n",
        "    attn_weights = torch.sum(attn_weights, dim=2) #- [BATCH, ENCODER_INPUT_SIZE]\n",
        "\n",
        "    coverage += attn_weights # [BATCH, ENCODER_INPUT_SIZE]\n",
        "\n",
        "    context_vector = attn_weights.view(BATCH_SIZE, ENCODER_INPUT_SIZE, 1) * encoder_outputs # [BATCH, ENCODER_INPUT_SIZE, HIDDEN]\n",
        "\n",
        "    context_vector = torch.sum(context_vector, dim=1) # [BATCH, HIDDEN]\n",
        "\n",
        "    return context_vector, attn_weights, coverage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "hH3FehyM_nXk"
      },
      "outputs": [],
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "  def __init__(self, output_vocab_size, hidden_size, embedding_size):\n",
        "    super(AttnDecoderRNN, self).__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_size = embedding_size\n",
        "    self.calcAttn = AttnCalc(hidden_size).to(device)\n",
        "\n",
        "    self.embedding = nn.Embedding(self.output_vocab_size, embedding_size)\n",
        "\n",
        "    self.preOut = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "    self.out = nn.Linear(self.hidden_size, self.output_vocab_size)\n",
        "    self.newIn = nn.Linear(self.hidden_size + embedding_size, self.hidden_size)\n",
        "\n",
        "    self.dropout = nn.Dropout(0.1)\n",
        "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    self.tanhout = nn.Tanh()\n",
        "\n",
        "    self.decoder_times = [0, 0, 0, 0, 0]\n",
        "\n",
        "  def forward(self, encoder_outputs, input, hidden, coverage, context_vector=None):\n",
        "    # --------------------------------\n",
        "    start = time.time()\n",
        "    embedded = self.embedding(input).view(1, BATCH_SIZE, -1) # [1, BATCH, EMBEDDING]\n",
        "    embedded = self.dropout(embedded) # [1, BATCH, EMBEDDING]\n",
        "    embedded = embedded.squeeze(0) # [BATCH, EMBEDDING] NOTE: da chiedere\n",
        "    self.decoder_times[0] += time.time() - start\n",
        "    # --------------------------------\n",
        "\n",
        "\n",
        "    # --------------------------------\n",
        "    start = time.time()\n",
        "\n",
        "    # hidden = [1, BATCH, HIDDEN]\n",
        "    # encoder_outputs = [BATCH, ENCODER_INPUT_SIZE, HIDDEN]\n",
        "    # coverage = [BATCH, ENCODER_INPUT_SIZE]\n",
        "\n",
        "    if context_vector is None:\n",
        "      context_vector, _, _ = self.calcAttn(hidden, encoder_outputs, coverage)\n",
        "    #context_vector = [BATCH, HIDDEN]\n",
        "\n",
        "    self.decoder_times[1] += time.time() - start\n",
        "    # --------------------------------\n",
        "\n",
        "    # --------------------------------\n",
        "    start = time.time()\n",
        "    \n",
        "    # input -> [BATCH, EMBEDDING + HIDDEN]\n",
        "    new_input = self.newIn(torch.cat((embedded, context_vector), 1)) #- [BATCH, HIDDEN]\n",
        "\n",
        "    output, hidden = self.gru(new_input.view(1, BATCH_SIZE, -1), hidden) # [1, BATCH, HIDDEN]\n",
        "    output = output.squeeze(0) #- [BATCH, HIDDEN]\n",
        "\n",
        "    self.decoder_times[2] += time.time() - start\n",
        "    # --------------------------------\n",
        "\n",
        "    # --------------------------------\n",
        "    start = time.time()\n",
        "\n",
        "    context_vector, attn_weights, coverage = self.calcAttn(hidden, encoder_outputs, coverage)\n",
        "    #- coverage -> [BATCH, ENCODER_INPUT_SIZE]\n",
        "    #- context_vector -> [BATCH, HIDDEN]\n",
        "    #- attn_weights -> [BATCH, ENCODER_INPUT_SIZE]\n",
        "\n",
        "    output = torch.cat((output, context_vector), 1) # [BATCH, HIDDEN * 2]\n",
        "    output = self.preOut(output) # [BATCH, HIDDEN]\n",
        "    output = self.tanhout(output) #- [BATCH, HIDDEN] NOTE: da chiedere\n",
        "\n",
        "    self.decoder_times[3] += time.time() - start\n",
        "    # --------------------------------\n",
        "\n",
        "    # output = self.dropout2(output) 0.5\n",
        "\n",
        "    # --------------------------------\n",
        "    start = time.time()\n",
        "\n",
        "    output = self.out(output) # [BATCH, OUTPUT_VOCAB_SIZE]\n",
        "\n",
        "    output = F.log_softmax(output, dim=1) #- [BATCH, OUTPUT_VOCAB_SIZE]\n",
        "\n",
        "    self.decoder_times[4] += time.time() - start\n",
        "    # --------------------------------\n",
        "\n",
        "    return output, hidden, context_vector, attn_weights, coverage\n",
        "\n",
        "  def initHidden(self):\n",
        "    return torch.zeros(1, 1, self.hidden_size, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kt-LrRmd_nXo"
      },
      "source": [
        "# TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "OyRbCtGW_nXo"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "  encoder_hidden = encoder.initHidden()\n",
        "\n",
        "  encoder_optimizer.zero_grad()\n",
        "  decoder_optimizer.zero_grad()\n",
        "\n",
        "  input_length = input_tensor.size(2) # == PHRASE_SIZE\n",
        "  target_length = target_tensor.size(1)\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  encoder_outputs = torch.zeros(ENCODER_INPUT_SIZE, BATCH_SIZE, encoder.hidden_size, device=device)\n",
        "\n",
        "  for i in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[:,:,i], encoder_hidden) # [1, BATCH, HIDDEN]\n",
        "    encoder_outputs[i] = encoder_output[0]\n",
        "\n",
        "  encoder_outputs = encoder_outputs.permute(1, 0, 2) # [BATCH, ENCODER_INPUT_SIZE, HIDDEN]\n",
        "\n",
        "  decoder_input = torch.tensor([type_vocab.getID(START_TOKEN) for _ in range(BATCH_SIZE)], device=device)\n",
        "\n",
        "  decoder_hidden = encoder_hidden\n",
        "  coverage = torch.zeros(BATCH_SIZE, ENCODER_INPUT_SIZE, device=device)\n",
        "  context_vector = None\n",
        "\n",
        "  start = time.time()\n",
        "\n",
        "  for di in range(target_length):\n",
        "    start_cycle = time.time()\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    # use_teacher_forcing = True\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "      # Teacher forcing: Feed the target as the next input\n",
        "      decoder_output, decoder_hidden, context_vector, attn_weights, coverage = decoder(encoder_outputs, decoder_input, decoder_hidden, coverage, context_vector)\n",
        "      decoder_input = target_tensor[:, di]  # Teacher forcing\n",
        "        \n",
        "    else:\n",
        "      # Without teacher forcing: use its own predictions as the next input\n",
        "      decoder_output, decoder_hidden, context_vector, attn_weights, coverage = decoder(encoder_outputs, decoder_input, decoder_hidden, coverage, context_vector)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "      decoder_input = topi.squeeze()\n",
        "\n",
        "    loss += criterion(decoder_output, target_tensor[:,di])\n",
        "\n",
        "    print(f\"iter time {di}: {asMsecs(time.time() - start_cycle)}\")\n",
        "\n",
        "\n",
        "  # for i in range(len(decoder.decoder_times)):\n",
        "  #   print(f\"avg {i} = {asMsecs(decoder.decoder_times[i] / len(train_pairs))}\")\n",
        "  #   print(f\"tot {i} = {asMsecs(decoder.decoder_times[i])}\")\n",
        "  #   print(\"------------------\")\n",
        "\n",
        "  print(f\"train decoder time: {asMsecs(time.time() - start)}\")\n",
        "\n",
        "  raise Exception(\"stop\") # solo per debug\n",
        "  \n",
        "  start = time.time()\n",
        "\n",
        "  loss = loss / target_length\n",
        "  loss.backward()\n",
        "\n",
        "  encoder_optimizer.step()\n",
        "  decoder_optimizer.step()\n",
        "\n",
        "  print(f\"train backward time: {asMsecs(time.time() - start)}\")\n",
        "  start = time.time()\n",
        "\n",
        "\n",
        "  return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "F0tTZx4G_nXp"
      },
      "outputs": [],
      "source": [
        "def trainEpoch(encoder, decoder, inputs, print_times=10, plot_times=10000, learning_rate=5e-5):\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  print_loss_total = 0  # Reset every print_every\n",
        "  plot_loss_total = 0  # Reset every plot_every\n",
        "  epoch_len = len(inputs)\n",
        "  plot_every = max(int(epoch_len / plot_times), 1)\n",
        "  print_every = max(int(epoch_len / print_times), 1)\n",
        "\n",
        "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  for iter in range(1, epoch_len+1):\n",
        "    # ogni elemento di inputs è una tupla (input, target)\n",
        "    # ogni valore input è un tensore di dimensione [3, batch, encoder_input_size], deve 3 rappresenta (tipo, valore, posizione)\n",
        "    # ogni valore target è un tensore di dimensione [batch, decoder_output_size]\n",
        "\n",
        "    training_pair = inputs[iter-1]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "\n",
        "    print(f\"iter {iter} time: {timeSince(start_time, 1)}\")\n",
        "\n",
        "    print_loss_total += loss\n",
        "    plot_loss_total += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "      print_loss_avg = print_loss_total / print_every\n",
        "      print_loss_total = 0\n",
        "      print(f\"{timeSince(start, iter / epoch_len+1)} ({iter} {iter / (epoch_len+1) * 100:.2f}%) {print_loss_avg:.4f}\")\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "      plot_loss_avg = plot_loss_total / plot_every\n",
        "      plot_losses.append(plot_loss_avg)\n",
        "      plot_loss_total = 0\n",
        "\n",
        "  showPlot(plot_losses)\n",
        "  return getPlot(plot_losses)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73iWON5L_nXp"
      },
      "source": [
        "# EVALUATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "iu-24Dm3_nXq"
      },
      "outputs": [],
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "def evaluate(input_tensor, target_tensor, encoder, decoder, criterion):\n",
        "  encoder_hidden = encoder.initHidden()\n",
        "\n",
        "  input_length = input_tensor.size(2) # == PHRASE_SIZE\n",
        "  target_length = target_tensor.size(1)\n",
        "\n",
        "  loss = 0\n",
        "\n",
        "  encoder_outputs = torch.zeros(ENCODER_INPUT_SIZE, BATCH_SIZE, encoder.hidden_size, device=device)\n",
        "\n",
        "  for i in range(input_length):\n",
        "    encoder_output, encoder_hidden = encoder(input_tensor[:,:,i], encoder_hidden) # [1, BATCH, HIDDEN]\n",
        "    encoder_outputs[i] = encoder_output[0]\n",
        "\n",
        "  encoder_outputs = encoder_outputs.permute(1, 0, 2) # [BATCH, ENCODER_INPUT_SIZE, HIDDEN]\n",
        "\n",
        "  decoder_input = torch.tensor([[type_vocab.getID(START_TOKEN)] for _ in range(BATCH_SIZE)], device=device)\n",
        "  decoder_input = decoder_input.view(1, BATCH_SIZE)\n",
        "\n",
        "  decoder_hidden = encoder_hidden\n",
        "  coverage = torch.zeros(BATCH_SIZE, ENCODER_INPUT_SIZE, device=device)\n",
        "  context_vector = None\n",
        "\n",
        "  decoder_outputs = []\n",
        "\n",
        "  for di in range(target_length):\n",
        "    decoder_output, decoder_hidden, context_vector, attn_weights, coverage = decoder(encoder_outputs, decoder_input, decoder_hidden, coverage, context_vector)\n",
        "    topv, topi = decoder_output.topk(1)\n",
        "    decoder_input = topi\n",
        "\n",
        "    loss += criterion(decoder_output, target_tensor[:,di])\n",
        "    decoder_outputs.append(decoder_output)\n",
        "\n",
        "  loss = loss / target_length\n",
        "\n",
        "  return loss.item(), decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluateEpoch(encoder, decoder, inputs):\n",
        "  start = time.time()\n",
        "  plot_losses = []\n",
        "  loss_total = 0  # Reset every print_every\n",
        "  epoch_len = len(inputs)\n",
        "\n",
        "  criterion = nn.NLLLoss()\n",
        "\n",
        "  sample_start = None\n",
        "  sample_end = None\n",
        "\n",
        "  for iter in range(1, epoch_len + 1):\n",
        "    training_pair = inputs[iter-1]\n",
        "    input_tensor = training_pair[0]\n",
        "    target_tensor = training_pair[1]\n",
        "\n",
        "    loss, decoder_outputs = evaluate(input_tensor, target_tensor, encoder, decoder, criterion)\n",
        "    loss_total += loss\n",
        "\n",
        "    if iter == 0:\n",
        "      sample_start = [decoder_outputs, target_tensor]\n",
        "    if iter == epoch_len - 1:\n",
        "      sample_end = [decoder_outputs, target_tensor]\n",
        "  \n",
        "  avg_loss = loss_total / epoch_len\n",
        "  return avg_loss, sample_start, sample_end\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggT9p1OY_nXq"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "yog4T4XL_nXr"
      },
      "outputs": [],
      "source": [
        "\n",
        "encoder = EncoderRNN(type_vocab, value_vocab, 256, 128).to(device)\n",
        "decoder = AttnDecoderRNN(len(token_vocab), 256, 128).to(device)\n",
        "\n",
        "# decoder = torch.load(\"saved_models/decoder_24.08_14.55-20000-iters.pt\", map_location=device)\n",
        "# encoder = torch.load(\"saved_models/encoder_24.08_14.55-20000-iters.pt\", map_location=device)\n",
        "\n",
        "done_epochs = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "gA9cCWIW_nXr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----========= EPOCH 1/1=========----\n",
            "iter time 0: 0s 58ms\n",
            "iter time 1: 0s 1ms\n",
            "iter time 2: 0s 1ms\n",
            "iter time 3: 0s 1ms\n",
            "iter time 4: 0s 0ms\n",
            "iter time 5: 0s 1ms\n",
            "iter time 6: 0s 1ms\n",
            "iter time 7: 0s 0ms\n",
            "iter time 8: 0s 3ms\n",
            "iter time 9: 0s 1ms\n",
            "iter time 10: 0s 1ms\n",
            "iter time 11: 0s 0ms\n",
            "iter time 12: 0s 1ms\n",
            "iter time 13: 0s 0ms\n",
            "iter time 14: 0s 1ms\n",
            "iter time 15: 0s 1ms\n",
            "iter time 16: 0s 1ms\n",
            "iter time 17: 0s 1ms\n",
            "iter time 18: 0s 1ms\n",
            "iter time 19: 0s 0ms\n",
            "iter time 20: 0s 1ms\n",
            "iter time 21: 0s 1ms\n",
            "iter time 22: 0s 2ms\n",
            "iter time 23: 0s 1ms\n",
            "iter time 24: 0s 4ms\n",
            "iter time 25: 0s 2ms\n",
            "iter time 26: 0s 2ms\n",
            "iter time 27: 0s 1ms\n",
            "iter time 28: 0s 1ms\n",
            "iter time 29: 0s 16ms\n",
            "iter time 30: 0s 46ms\n",
            "iter time 31: 0s 32ms\n",
            "iter time 32: 0s 44ms\n",
            "iter time 33: 0s 43ms\n",
            "iter time 34: 0s 30ms\n",
            "train decoder time: 0s 332ms\n"
          ]
        },
        {
          "ename": "Exception",
          "evalue": "stop",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\tomma\\PROGRAMMING_C\\PYTORCH\\wikiauto3\\model.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m epoch_start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m random\u001b[39m.\u001b[39mshuffle(pairs)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m plot \u001b[39m=\u001b[39m trainEpoch(encoder, decoder, train_pairs, print_times\u001b[39m=\u001b[39;49mPRINT_TIMES, plot_times\u001b[39m=\u001b[39;49mPLOT_TIMES)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m------------------- Trained -------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m curr_loss, sample_start, sample_end \u001b[39m=\u001b[39m evaluateEpoch(encoder, decoder, test_pairs)\n",
            "\u001b[1;32mc:\\Users\\tomma\\PROGRAMMING_C\\PYTORCH\\wikiauto3\\model.ipynb Cell 18\u001b[0m in \u001b[0;36mtrainEpoch\u001b[1;34m(encoder, decoder, inputs, print_times, plot_times, learning_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m target_tensor \u001b[39m=\u001b[39m training_pair[\u001b[39m1\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39miter \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39miter\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m time: \u001b[39m\u001b[39m{\u001b[39;00mtimeSince(start_time, \u001b[39m1\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m print_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
            "\u001b[1;32mc:\\Users\\tomma\\PROGRAMMING_C\\PYTORCH\\wikiauto3\\model.ipynb Cell 18\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m# for i in range(len(decoder.decoder_times)):\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m#   print(f\"avg {i} = {asMsecs(decoder.decoder_times[i] / len(train_pairs))}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m#   print(f\"tot {i} = {asMsecs(decoder.decoder_times[i])}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m#   print(\"------------------\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain decoder time: \u001b[39m\u001b[39m{\u001b[39;00masMsecs(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mstop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/tomma/PROGRAMMING_C/PYTORCH/wikiauto3/model.ipynb#X25sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m/\u001b[39m target_length\n",
            "\u001b[1;31mException\u001b[0m: stop"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "from math import ceil\n",
        "import os\n",
        "\n",
        "EPOCHS = 1\n",
        "FLAT = 3\n",
        "\n",
        "PLOT_TIMES = 1000\n",
        "PRINT_TIMES = 5\n",
        "BATCH_PRINT_SIZE = 5\n",
        "SAVE_MODEL_EVERY = 1\n",
        "SAVE_PLOT_EVERY = 1\n",
        "\n",
        "start_time = str(datetime.now().strftime(\"%d.%m_%H.%M\"))\n",
        "output_file = f\"{base_path}/output/out-{start_time}.txt\"\n",
        "\n",
        "# with open(output_file, 'w', encoding='utf-8') as outfile: pass\n",
        "\n",
        "prec_loss = 0\n",
        "\n",
        "def saveModel(encoder, decoder, epoch):\n",
        "  torch.save(encoder, f\"{base_path}/models/encoder_{start_time}-ep_{epoch}-iters.pt\")\n",
        "  torch.save(decoder, f\"{base_path}/models/decoder_{start_time}-ep_{epoch}-iters.pt\")\n",
        "\n",
        "def savePlot(plot, epoch):\n",
        "  plot.savefig(f\"{base_path}/plots/plot_{start_time}-ep_{epoch}-iters.png\")\n",
        "\n",
        "def saveOutput(output, target, epoch):\n",
        "  with open(output_file, 'a', encoding='utf-8') as outfile:\n",
        "    for i in range(min(BATCH_PRINT_SIZE, len(output))):\n",
        "      outfile.write(\"----------------------\\n\")\n",
        "      predict = \"\"\n",
        "      target = \"\"\n",
        "      for word in output[i]:\n",
        "        predict += str(word) + \" \"\n",
        "      outfile.write(predict + \"\\n\")\n",
        "      # outfile.write(\"-.... ↑|predict|↑ ....... ↓|target|↓ ....-\\n\")\n",
        "      # for word in pairs[0][1][i]:\n",
        "      #   target += token_vocab.getWord(word.item()) + \" \"\n",
        "      # outfile.write(target + \"\\n\")\n",
        "    \n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "  print(f\"----========= EPOCH {epoch}/{EPOCHS}=========----\")\n",
        "  epoch_start = time.time()\n",
        "  \n",
        "  random.shuffle(pairs)\n",
        "  plot = trainEpoch(encoder, decoder, train_pairs, print_times=PRINT_TIMES, plot_times=PLOT_TIMES)\n",
        "  print(f\"------------------- Trained -------------------\")\n",
        "  curr_loss, sample_start, sample_end = evaluateEpoch(encoder, decoder, test_pairs)\n",
        "\n",
        "  temp_loss = calc_avg_loss(prec_loss, curr_loss)\n",
        "  if(prec_loss < temp_loss):\n",
        "    FLAT -= 1\n",
        "  else:\n",
        "    FLAT = 3\n",
        "  \n",
        "  if(FLAT == 0):\n",
        "    break\n",
        "  \n",
        "  prec_loss = temp_loss\n",
        "\n",
        "  print(f\"------------------- Finished epoch -------------------\")\n",
        "  print(f\"time: {int((time.time() - epoch_start)/60)}min\")\n",
        "  print(f\"loss: {curr_loss}, avg loss: {temp_loss}, flat: {FLAT}\")\n",
        "\n",
        "  # saveOutput(sample_start[0], sample_start[1], epoch)\n",
        "  # saveOutput(sample_end[0], sample_end[1], epoch)\n",
        "\n",
        "  # if epoch % SAVE_PLOT_EVERY == 0:\n",
        "  #   savePlot(plot, epoch, epoch)\n",
        "\n",
        "  # if epoch % SAVE_MODEL_EVERY == 0:\n",
        "  #   saveModel(encoder, decoder, epoch, epoch)\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ4n4HwB_nXr"
      },
      "outputs": [],
      "source": [
        "# encoder = torch.load(\"saved_models/encoder_100000-iters.pt\")\n",
        "# decoder = torch.load(\"saved_models/decoder_100000-iters.pt\")\n",
        "\n",
        "# test_input = pairs[0][0]\n",
        "\n",
        "# outputs, _ = evaluate(encoder, decoder, test_input)\n",
        "# for i in range(len(outputs)):\n",
        "#   print(\"----------------------\")\n",
        "#   predict = \"\"\n",
        "#   target = \"\"\n",
        "#   for word in outputs[i]:\n",
        "#     predict += str(word) + \" \"\n",
        "#   print(predict)\n",
        "#   print(\"-.... ↑|predict|↑ ....... ↓|target|↓ ....-\")\n",
        "#   for word in pairs[0][1][i]:\n",
        "#     target += token_vocab.getWord(word.item()) + \" \"\n",
        "#   print(target)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cf21a6fff9624ce691a4fad43c68540179d36da31fc4f80138ac9dffe05c7a89"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
