{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from vocab import vocab, END_TOKEN, START_TOKEN, PADDING_TOKEN, UNKNOWN_TOKEN\n",
    "from helpers import readLines\n",
    "from load_data import load_data_evaluate, load_data_training\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import json\n",
    "\n",
    "PHRASE_SIZE = 20\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, type_vocab, value_vocab, hidden_size, embedding_size):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.typeEmbedding = nn.Embedding(len(type_vocab), embedding_size, device=device)\n",
    "    self.valueEmbedding = nn.Embedding(len(value_vocab), embedding_size, device=device)\n",
    "    self.positionEmbedding = nn.Embedding(PHRASE_SIZE, 10, device=device)\n",
    "    \n",
    "    self.gru = nn.GRU(embedding_size * 2 + 10, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    E_type_out = self.typeEmbedding(inputs[0])\n",
    "    E_value_out = self.valueEmbedding(inputs[1])\n",
    "    E_pos_out = self.positionEmbedding(inputs[2])\n",
    "\n",
    "    output = torch.cat((E_type_out, E_pos_out, E_value_out), dim=1).view(1, BATCH_SIZE, -1)\n",
    "\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size):\n",
    "    super(DecoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.embedding = nn.Embedding(output_size, hidden_size, device=device)\n",
    "    \n",
    "    self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    self.out = nn.Linear(hidden_size, output_size)\n",
    "    self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, input, hidden):\n",
    "    output = self.embedding(input)\n",
    "\n",
    "    output = F.relu(output)\n",
    "    \n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    output = self.softmax(self.out(output[0]))\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=PHRASE_SIZE):\n",
    "    super(AttnDecoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_size = output_size\n",
    "    self.dropout_p = dropout_p\n",
    "    self.max_length = max_length\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "    self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "    self.dropout = nn.Dropout(self.dropout_p)\n",
    "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "    self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "  def forward(self, input, hidden, encoder_outputs):\n",
    "    embedded = self.embedding(input).view(1, BATCH_SIZE, -1)\n",
    "    embedded = self.dropout(embedded)\n",
    "\n",
    "    # print(embedded.shape, hidden.shape)\n",
    "    # print(embedded[0].shape, hidden[0].shape)\n",
    "\n",
    "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "    attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "    output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "\n",
    "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "    return output, hidden, attn_weights\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMING & PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "  m = math.floor(s / 60)\n",
    "  s -= m * 60\n",
    "  return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "  now = time.time()\n",
    "  s = now - since\n",
    "  es = s / (percent)\n",
    "  rs = es - s\n",
    "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # this locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=0.5)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  plt.plot(points)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too many pairs requested, new pair amount (articles / batch_size):  78523\n",
      "------------------------\n",
      "loading data: 7853/78523 (10.000891458553545%)\n",
      "loading data: 15706/78523 (20.00178291710709%)\n",
      "loading data: 23559/78523 (30.002674375660632%)\n",
      "loading data: 31412/78523 (40.00356583421418%)\n",
      "loading data: 39265/78523 (50.004457292767725%)\n",
      "loading data: 47118/78523 (60.005348751321264%)\n",
      "loading data: 54971/78523 (70.00624020987482%)\n",
      "loading data: 62824/78523 (80.00713166842836%)\n",
      "loading data: 70677/78523 (90.00802312698191%)\n",
      "------------------------\n",
      "pairs:  78496\n",
      "batch size: 5, phrase size: 20\n",
      "input shape:  torch.Size([3, 5, 20])\n",
      "output shape:  torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "type_vocab, value_vocab, token_vocab, pairs = load_data_training(torch, device, 50000, 5, 20, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# NOTE: inputs are (1, BATCH_SIZE)\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "  encoder_hidden = encoder.initHidden()\n",
    "\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_length = input_tensor.size(2) # == PHRASE_SIZE\n",
    "  target_length = target_tensor.size(1)\n",
    "\n",
    "  # NOTE: attention\n",
    "  encoder_outputs = torch.zeros(PHRASE_SIZE, encoder.hidden_size, device=device)\n",
    "\n",
    "  loss = 0\n",
    "\n",
    "  for i in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[:,:,i], encoder_hidden)\n",
    "    # NOTE: attention\n",
    "    encoder_outputs[i] = encoder_output[0, 0]\n",
    "\n",
    "  decoder_input = torch.tensor([[type_vocab.getID(START_TOKEN)] for _ in range(BATCH_SIZE)], device=device)\n",
    "  decoder_input = decoder_input.view(1, BATCH_SIZE)\n",
    "\n",
    "  decoder_hidden = encoder_hidden\n",
    "\n",
    "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "  # use_teacher_forcing = True\n",
    "\n",
    "  if use_teacher_forcing:\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(target_length):\n",
    "      # NOTE: attention\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "      loss += criterion(decoder_output, target_tensor[:,di])\n",
    "      decoder_input = target_tensor[:, di].view(1, BATCH_SIZE)  # Teacher forcing\n",
    "      \n",
    "  else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "      # NOTE: attention\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "      topv, topi = decoder_output.topk(1)\n",
    "      decoder_input = topi.squeeze().detach().view(1, BATCH_SIZE)  # detach from history as input\n",
    "\n",
    "      loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "  loss.backward()\n",
    "\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=5e-5):\n",
    "  start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0  # Reset every print_every\n",
    "  plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  training_pairs = [random.choice(pairs) for i in range(n_iters)]\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for iter in range(1, n_iters + 1):\n",
    "    training_pair = training_pairs[iter - 1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "      print_loss_avg = print_loss_total / print_every\n",
    "      print_loss_total = 0\n",
    "      print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                      iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "      plot_loss_avg = plot_loss_total / plot_every\n",
    "      plot_losses.append(plot_loss_avg)\n",
    "      plot_loss_total = 0\n",
    "\n",
    "  \n",
    "  showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2m 17s (- 20m 35s) (1000 10%) 7.4212\n",
      "4m 21s (- 17m 25s) (2000 20%) 6.7440\n",
      "6m 27s (- 15m 4s) (3000 30%) 6.6572\n",
      "8m 33s (- 12m 50s) (4000 40%) 6.5830\n",
      "10m 40s (- 10m 40s) (5000 50%) 6.5033\n",
      "12m 48s (- 8m 32s) (6000 60%) 6.4785\n",
      "14m 55s (- 6m 23s) (7000 70%) 6.4487\n",
      "17m 3s (- 4m 15s) (8000 80%) 6.3748\n",
      "19m 9s (- 2m 7s) (9000 90%) 6.3438\n",
      "21m 18s (- 0m 0s) (10000 100%) 6.3140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "encoder = EncoderRNN(type_vocab, value_vocab, 256, 128).to(device)\n",
    "# decoder = DecoderRNN(256, len(token_vocab)).to(device)\n",
    "decoder = AttnDecoderRNN(256, len(token_vocab)).to(device)\n",
    "\n",
    "trainIters(encoder, decoder, 10000, print_every=1000, plot_every=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected 1000 pairs out of 176796 available\n",
      "------------------------\n",
      "loading data: 100/1000 (10.0%)\n",
      "loading data: 200/1000 (20.0%)\n",
      "loading data: 300/1000 (30.0%)\n",
      "loading data: 400/1000 (40.0%)\n",
      "loading data: 500/1000 (50.0%)\n",
      "loading data: 600/1000 (60.0%)\n",
      "loading data: 700/1000 (70.0%)\n",
      "loading data: 800/1000 (80.0%)\n",
      "loading data: 900/1000 (90.0%)\n",
      "loading data: 1000/1000 (100.0%)\n",
      "------------------------\n",
      "pairs:  1000\n",
      "batch size: 5, phrase size: 20\n",
      "input shape:  torch.Size([5, 20])\n",
      "output shape:  torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "type_vocab, value_vocab, token_vocab, inputs = load_data_evaluate(torch, device, 50000, BATCH_SIZE, PHRASE_SIZE, 1000)\n",
    "\n",
    "# FIXME: not implemented yet\n",
    "def evaluate(encoder, decoder, input_tensor, max_length=PHRASE_SIZE):\n",
    "  with torch.no_grad():\n",
    "\n",
    "    input_length = input_tensor.size()[0]\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "      encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
    "      encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[START_TOKEN]], device=device)  # SOS\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "    for di in range(max_length):\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      decoder_attentions[di] = decoder_attention.data\n",
    "      topv, topi = decoder_output.data.topk(1)\n",
    "      if topi.item() == END_TOKEN:\n",
    "        decoded_words.append('<EOS>')\n",
    "        break\n",
    "      else:\n",
    "        decoded_words.append(token_vocab.index2word[topi.item()])\n",
    "\n",
    "      decoder_input = topi.squeeze().detach()\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf21a6fff9624ce691a4fad43c68540179d36da31fc4f80138ac9dffe05c7a89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
