{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from vocab import vocab, END_TOKEN, START_TOKEN, PADDING_TOKEN, UNKNOWN_TOKEN\n",
    "from helpers import readLines\n",
    "from load_data import load_data_evaluate, load_data_training, getInputSizeAverage\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import json\n",
    "\n",
    "ENCODER_INPUT_SIZE = 20\n",
    "DECODER_OUTPUT_SIZE = 75\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device: {device}\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19.0, 19.0, 74.0)\n"
     ]
    }
   ],
   "source": [
    "# from load_data import getInputSizeAverage\n",
    "\n",
    "# print(getInputSizeAverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "  def __init__(self, type_vocab, value_vocab, hidden_size, embedding_size):\n",
    "    super(EncoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "\n",
    "    self.typeEmbedding = nn.Embedding(len(type_vocab), embedding_size, device=device)\n",
    "    self.valueEmbedding = nn.Embedding(len(value_vocab), embedding_size, device=device)\n",
    "    self.positionEmbedding = nn.Embedding(ENCODER_INPUT_SIZE, 10, device=device)\n",
    "    \n",
    "    self.gru = nn.GRU(embedding_size * 2 + 10, self.hidden_size)\n",
    "\n",
    "  def forward(self, inputs, hidden):\n",
    "    E_type_out = self.typeEmbedding(inputs[0])\n",
    "    E_value_out = self.valueEmbedding(inputs[1])\n",
    "    E_pos_out = self.positionEmbedding(inputs[2])\n",
    "\n",
    "    output = torch.cat((E_type_out, E_pos_out, E_value_out), dim=1).view(1, BATCH_SIZE, -1)\n",
    "\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "    return output, hidden\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, BATCH_SIZE, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ATTENTION DECODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "  def __init__(self, hidden_size, output_vocab_size, dropout_p=0.1):\n",
    "    super(AttnDecoderRNN, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.dropout_p = dropout_p\n",
    "\n",
    "    self.embedding = nn.Embedding(self.output_vocab_size, self.hidden_size)\n",
    "    self.attn = nn.Linear(self.hidden_size * 2, ENCODER_INPUT_SIZE)\n",
    "    self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "    self.dropout = nn.Dropout(self.dropout_p)\n",
    "    self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "    self.out = nn.Linear(self.hidden_size, self.output_vocab_size)\n",
    "\n",
    "  def forward(self, input, hidden, encoder_outputs):\n",
    "    embedded = self.embedding(input).view(1, BATCH_SIZE, -1)\n",
    "    embedded = self.dropout(embedded)\n",
    "\n",
    "    attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "\n",
    "    attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0))\n",
    "\n",
    "    output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "    output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "    output = F.relu(output)\n",
    "    output, hidden = self.gru(output, hidden)\n",
    "\n",
    "    output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "    return output, hidden, attn_weights\n",
    "\n",
    "  def initHidden(self):\n",
    "    return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIMING & PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "def asMinutes(s):\n",
    "  m = math.floor(s / 60)\n",
    "  s -= m * 60\n",
    "  return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "  now = time.time()\n",
    "  s = now - since\n",
    "  es = s / (percent)\n",
    "  rs = es - s\n",
    "  return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # this locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=0.01)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  # ax.set_facecolor('pink')\n",
    "  plt.plot(points)\n",
    "  plt.show()\n",
    "\n",
    "def getPlot(points):\n",
    "  plt.figure()\n",
    "  fig, ax = plt.subplots()\n",
    "  # this locator puts ticks at regular intervals\n",
    "  loc = ticker.MultipleLocator(base=0.01)\n",
    "  ax.yaxis.set_major_locator(loc)\n",
    "  # ax.set_facecolor('pink')\n",
    "  plt.plot(points)\n",
    "  return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_vocab, value_vocab, token_vocab, pairs = load_data_training(\n",
    "  torch=torch,\n",
    "  device=device,\n",
    "  vocab_size=50000,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  input_size=ENCODER_INPUT_SIZE,\n",
    "  output_size=DECODER_OUTPUT_SIZE,\n",
    "  pair_amount=1000000/BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "# NOTE: inputs are (1, BATCH_SIZE)\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "  encoder_hidden = encoder.initHidden()\n",
    "\n",
    "  encoder_optimizer.zero_grad()\n",
    "  decoder_optimizer.zero_grad()\n",
    "\n",
    "  input_length = input_tensor.size(2) # == PHRASE_SIZE\n",
    "  target_length = target_tensor.size(1)\n",
    "\n",
    "  # NOTE: attention\n",
    "  encoder_outputs = torch.zeros(ENCODER_INPUT_SIZE, encoder.hidden_size, device=device)\n",
    "\n",
    "  loss = 0\n",
    "\n",
    "  for i in range(input_length):\n",
    "    encoder_output, encoder_hidden = encoder(input_tensor[:,:,i], encoder_hidden)\n",
    "    # NOTE: attention\n",
    "    encoder_outputs[i] = encoder_output[0, 0]\n",
    "\n",
    "  decoder_input = torch.tensor([[type_vocab.getID(START_TOKEN)] for _ in range(BATCH_SIZE)], device=device)\n",
    "  decoder_input = decoder_input.view(1, BATCH_SIZE)\n",
    "\n",
    "  decoder_hidden = encoder_hidden\n",
    "\n",
    "  use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "  # use_teacher_forcing = False\n",
    "\n",
    "  if use_teacher_forcing:\n",
    "    # Teacher forcing: Feed the target as the next input\n",
    "    for di in range(target_length):\n",
    "      # NOTE: attention\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "      loss += criterion(decoder_output, target_tensor[:,di])\n",
    "      decoder_input = target_tensor[:, di].view(1, BATCH_SIZE)  # Teacher forcing\n",
    "      \n",
    "  else:\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    for di in range(target_length):\n",
    "      # NOTE: attention\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      # decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "      topv, topi = decoder_output.topk(1)\n",
    "      # decoder_input = topi.squeeze().detach().view(1, BATCH_SIZE)  # detach from history as input\n",
    "      decoder_input = topi\n",
    "\n",
    "      loss += criterion(decoder_output, target_tensor[:,di])\n",
    "\n",
    "  # loss.backward()\n",
    "  loss = loss / target_length\n",
    "  # loss = loss / BATCH_SIZE\n",
    "  loss.backward()\n",
    "\n",
    "  encoder_optimizer.step()\n",
    "  decoder_optimizer.step()\n",
    "\n",
    "  return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=5e-5):\n",
    "  start = time.time()\n",
    "  plot_losses = []\n",
    "  print_loss_total = 0  # Reset every print_every\n",
    "  plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "  encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "  decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "  training_pairs = [random.choice(pairs) for i in range(n_iters)]\n",
    "  criterion = nn.NLLLoss()\n",
    "\n",
    "  for iter in range(1, n_iters + 1):\n",
    "    training_pair = training_pairs[iter - 1]\n",
    "    input_tensor = training_pair[0]\n",
    "    target_tensor = training_pair[1]\n",
    "\n",
    "    loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "\n",
    "    if iter % print_every == 0:\n",
    "      print_loss_avg = print_loss_total / print_every\n",
    "      print_loss_total = 0\n",
    "      print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                      iter, iter / n_iters * 100, print_loss_avg))\n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "      plot_loss_avg = plot_loss_total / plot_every\n",
    "      plot_losses.append(plot_loss_avg)\n",
    "      plot_loss_total = 0\n",
    "  \n",
    "  showPlot(plot_losses) \n",
    "  return getPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, input_tensor):\n",
    "  with torch.no_grad():\n",
    "    input_length = input_tensor.size(2) # == max_length\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_outputs = torch.zeros(ENCODER_INPUT_SIZE, encoder.hidden_size, device=device)\n",
    "\n",
    "    for ei in range(input_length):\n",
    "      encoder_output, encoder_hidden = encoder(input_tensor[:,:,ei], encoder_hidden)\n",
    "      encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[type_vocab.getID(START_TOKEN)] for _ in range(BATCH_SIZE)], device=device)\n",
    "    decoder_input = decoder_input.view(1, BATCH_SIZE)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    decoded_words = [[] for _ in range(BATCH_SIZE)]\n",
    "    decoder_attentions = torch.zeros(DECODER_OUTPUT_SIZE, DECODER_OUTPUT_SIZE)\n",
    "\n",
    "    for di in range(DECODER_OUTPUT_SIZE):\n",
    "      decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "      \n",
    "      # TODO: decoder attention\n",
    "      # decoder_attentions[di] = decoder_attention.data\n",
    "      topv, topi = decoder_output.data.topk(1)\n",
    "\n",
    "      # TODO: end tokens dopo fine della frase\n",
    "      for i in range(BATCH_SIZE):\n",
    "        decoded_words[i].append(token_vocab.getWord(topi[i].item()))\n",
    "\n",
    "      decoder_input = topi.squeeze().detach().view(1, BATCH_SIZE)  # detach from history as input\n",
    "\n",
    "    return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "encoder = EncoderRNN(type_vocab, value_vocab, 256, 128).to(device)\n",
    "decoder = AttnDecoderRNN(256, len(token_vocab)).to(device)\n",
    "\n",
    "# encoder = torch.load(\"saved_models/encoder_23.08_06.34-40000-iters.pt\", map_location=device)\n",
    "# decoder = torch.load(\"saved_models/decoder_23.08_06.34-40000-iters.pt\", map_location=device)\n",
    "\n",
    "done_epochs = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from math import ceil\n",
    "import os\n",
    "\n",
    "ITERS = 20000\n",
    "EPOCHS = 20\n",
    "SAVE_MODEL_EVERY = 1\n",
    "SAVE_PLOT_EVERY = 1\n",
    "PLOT_TIMES = 1000\n",
    "PRINT_TIMES = 5\n",
    "BATCH_PRINT_SIZE = 5\n",
    "\n",
    "test_input = pairs[0][0]\n",
    "\n",
    "start_time = str(datetime.now().strftime(\"%d.%m_%H.%M\"))\n",
    "output_file = f\"output/out-{start_time}.txt\"\n",
    "# output_file = \"output/out-23.08_01.18.txt\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "  pass\n",
    "\n",
    "for epoch in range(done_epochs, EPOCHS):\n",
    "  losses = [-1]\n",
    "\n",
    "  with open(output_file, 'a', encoding='utf-8') as outfile:\n",
    "    print(f\"----========= CYCLE {epoch+1}/{EPOCHS} - {ITERS*(epoch+1)} iterations =========----\")\n",
    "\n",
    "    plot = trainIters(encoder, decoder, ITERS, print_every=ceil(ITERS/PRINT_TIMES), plot_every=ceil(ITERS/PLOT_TIMES))\n",
    "    outputs, _ = evaluate(encoder, decoder, test_input)\n",
    "\n",
    "    now = datetime.now().strftime(\"%d.%m_%H.%M\")\n",
    "    \n",
    "    outfile.write(f\"\\n----========= CYCLE {epoch+1}/{EPOCHS} - {ITERS*(epoch+1)} iterations =========----\\n\\n\")\n",
    "  \n",
    "    for i in range(min(BATCH_PRINT_SIZE, len(outputs))):\n",
    "      outfile.write(\"----------------------\\n\")\n",
    "      predict = \"\"\n",
    "      target = \"\"\n",
    "      for word in outputs[i]:\n",
    "        predict += str(word) + \" \"\n",
    "      outfile.write(predict + \"\\n\")\n",
    "      outfile.write(\"-.... ↑|predict|↑ ....... ↓|target|↓ ....-\\n\")\n",
    "      for word in pairs[0][1][i]:\n",
    "        target += token_vocab.getWord(word.item()) + \" \"\n",
    "      outfile.write(target + \"\\n\")\n",
    "\n",
    "  if epoch % SAVE_PLOT_EVERY == 0:\n",
    "    plot.savefig(f\"plots/plot-{now}-{(epoch+1)*ITERS}-iters.png\", facecolor ='orange')\n",
    "\n",
    "  if epoch % SAVE_MODEL_EVERY == 0:\n",
    "    torch.save(encoder, f\"saved_models/encoder_{now}-{(epoch+1)*ITERS}-iters.pt\")\n",
    "    torch.save(decoder, f\"saved_models/decoder_{now}-{(epoch+1)*ITERS}-iters.pt\")\n",
    "\n",
    "# os.system('shutdown -s')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = torch.load(\"saved_models/encoder_100000-iters.pt\")\n",
    "decoder = torch.load(\"saved_models/decoder_100000-iters.pt\")\n",
    "\n",
    "test_input = pairs[0][0]\n",
    "\n",
    "outputs, _ = evaluate(encoder, decoder, test_input)\n",
    "for i in range(len(outputs)):\n",
    "  print(\"----------------------\")\n",
    "  predict = \"\"\n",
    "  target = \"\"\n",
    "  for word in outputs[i]:\n",
    "    predict += str(word) + \" \"\n",
    "  print(predict)\n",
    "  print(\"-.... ↑|predict|↑ ....... ↓|target|↓ ....-\")\n",
    "  for word in pairs[0][1][i]:\n",
    "    target += token_vocab.getWord(word.item()) + \" \"\n",
    "  print(target)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf21a6fff9624ce691a4fad43c68540179d36da31fc4f80138ac9dffe05c7a89"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
